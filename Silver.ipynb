{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c3aca6-3427-4411-9add-483ce6375ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required PySpark libraries for data processing\n",
    "from pyspark.sql import * # For Spark SQL functions like col, trim, filter, etc.\n",
    "from pyspark.sql.window import Window# For window functions to handle duplicates\n",
    "from pyspark.sql.functions import *    \n",
    "\n",
    "# Read the data from Bronze layer\n",
    "customers_df = spark.read.format(\"delta\").load(\"jaffle_shop_retail/bronze/raw_customers\")\n",
    "orders_df = spark.read.format(\"delta\").load(\"jaffle_shop_retail/bronze/raw_orders\")\n",
    "order_items_df = spark.read.format(\"delta\").load(\"jaffle_shop_retail/bronze/raw_order_items\")\n",
    "products_df = spark.read.format(\"delta\").load(\"jaffle_shop_retail/bronze/raw_products\")\n",
    "supplies_df = spark.read.format(\"delta\").load(\"jaffle_shop_retail/bronze/raw_supplies\")\n",
    "stores_df = spark.read.format(\"delta\").load(\"jaffle_shop_retail/bronze/raw_stores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b8af07-d9f3-4bff-9f50-e6e4f7fb2fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load customers_bronze from Unity Catalog\n",
    "customers_bronze = spark.table(\"jaffle_shop_retail.bronze.raw_customers\")\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, when, lit, sha2, current_timestamp\n",
    "\n",
    "# Step 1: Select and cast columns to proper data types\n",
    "customers_selected = customers_bronze.select(\n",
    "    col(\"id\").cast(StringType()).alias(\"customer_id\"),\n",
    "    col(\"name\").cast(StringType()).alias(\"customer_name\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter out null values from critical columns\n",
    "customers_no_nulls = customers_selected.filter(\n",
    "    col(\"customer_id\").isNotNull() &\n",
    "    col(\"customer_name\").isNotNull()\n",
    ")\n",
    "\n",
    "# Step 3: Handle missing customer names\n",
    "customers_with_name = customers_no_nulls.withColumn(\n",
    "    \"customer_name\",\n",
    "    when(col(\"customer_name\").isNull(), lit(\"Unknown Customer\"))\n",
    "    .otherwise(col(\"customer_name\"))\n",
    ")\n",
    "\n",
    "# Step 4: Remove duplicate customers\n",
    "customers_deduped = customers_with_name.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "# Step 5: Add customer_key hash column\n",
    "customers_with_key = customers_deduped.withColumn(\"customer_key\", sha2(col(\"customer_id\"), 256))\n",
    "\n",
    "# Step 6: Add loaded_at timestamp\n",
    "customers_silver_final = customers_with_key.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "# Step 7: Write to Silver schema\n",
    "customers_silver_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"jaffle_shop_retail.silver.customers\")\n",
    "\n",
    "# Step 8: Display final result\n",
    "display(customers_silver_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8850c8-1343-4d00-9dd2-95c209f2b981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load orders_bronze from Unity Catalog\n",
    "orders_bronze = spark.table(\"jaffle_shop_retail.bronze.raw_orders\")\n",
    "\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, when, lit, sha2, current_timestamp\n",
    "\n",
    "# Step 1: Select and cast columns to proper data types\n",
    "orders_selected = orders_bronze.select(\n",
    "    col(\"id\").cast(StringType()).alias(\"order_id\"),\n",
    "    col(\"customer\").cast(StringType()).alias(\"customer_id\"),\n",
    "    col(\"ordered_at\").cast(TimestampType()).alias(\"ordered_at\"),\n",
    "    col(\"store_id\").cast(StringType()).alias(\"store_id\"),\n",
    "    col(\"subtotal\").cast(DoubleType()).alias(\"subtotal\"),\n",
    "    col(\"tax_paid\").cast(DoubleType()).alias(\"tax_paid\"),\n",
    "    col(\"order_total\").cast(DoubleType()).alias(\"order_total\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter out null values from critical columns\n",
    "orders_no_nulls = orders_selected.filter(\n",
    "    col(\"order_id\").isNotNull() &\n",
    "    col(\"customer_id\").isNotNull() &\n",
    "    col(\"ordered_at\").isNotNull() &\n",
    "    col(\"store_id\").isNotNull()\n",
    ")\n",
    "\n",
    "# Step 3: Handle missing financial values\n",
    "orders_with_financials = orders_no_nulls.withColumn(\n",
    "    \"subtotal\",\n",
    "    when(col(\"subtotal\").isNull(), lit(0.0)).otherwise(col(\"subtotal\"))\n",
    ").withColumn(\n",
    "    \"tax_paid\",\n",
    "    when(col(\"tax_paid\").isNull(), lit(0.0)).otherwise(col(\"tax_paid\"))\n",
    ").withColumn(\n",
    "    \"order_total\",\n",
    "    when(col(\"order_total\").isNull(), lit(0.0)).otherwise(col(\"order_total\"))\n",
    ")\n",
    "\n",
    "# Step 4: Fix negative financial values\n",
    "orders_valid_financials = orders_with_financials.withColumn(\n",
    "    \"subtotal\",\n",
    "    when(col(\"subtotal\") < 0, lit(0.0)).otherwise(col(\"subtotal\"))\n",
    ").withColumn(\n",
    "    \"tax_paid\",\n",
    "    when(col(\"tax_paid\") < 0, lit(0.0)).otherwise(col(\"tax_paid\"))\n",
    ").withColumn(\n",
    "    \"order_total\",\n",
    "    when(col(\"order_total\") < 0, lit(0.0)).otherwise(col(\"order_total\"))\n",
    ")\n",
    "\n",
    "# Step 5: Remove duplicate orders\n",
    "orders_deduped = orders_valid_financials.dropDuplicates([\"order_id\"])\n",
    "\n",
    "# Step 6: Add order_key hash column\n",
    "orders_with_key = orders_deduped.withColumn(\"order_key\", sha2(col(\"order_id\"), 256))\n",
    "\n",
    "# Step 7: Add loaded_at timestamp\n",
    "orders_silver_final = orders_with_key.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "# Step 8: Write to Silver schema\n",
    "orders_silver_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"jaffle_shop_retail.silver.orders\")\n",
    "\n",
    "# Step 9: Display final result\n",
    "display(orders_silver_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c97738e0-00ce-4ac8-87af-e503be431b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load order_items_bronze from Unity Catalog\n",
    "order_items_bronze = spark.table(\"jaffle_shop_retail.bronze.raw_order_items\")\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, when, lit, sha2, concat, current_timestamp\n",
    "\n",
    "# Step 1: Select and cast columns to proper data types\n",
    "order_items_selected = order_items_bronze.select(\n",
    "    col(\"id\").cast(StringType()).alias(\"order_item_id\"),\n",
    "    col(\"order_id\").cast(StringType()).alias(\"order_id\"),\n",
    "    col(\"sku\").cast(StringType()).alias(\"product_sku\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter out null values from critical columns\n",
    "order_items_no_nulls = order_items_selected.filter(\n",
    "    col(\"order_item_id\").isNotNull() &\n",
    "    col(\"order_id\").isNotNull() &\n",
    "    col(\"product_sku\").isNotNull()\n",
    ")\n",
    "\n",
    "# Step 3: Handle missing SKUs\n",
    "order_items_with_sku = order_items_no_nulls.withColumn(\n",
    "    \"product_sku\",\n",
    "    when(col(\"product_sku\").isNull(), lit(\"UNKNOWN\"))\n",
    "    .otherwise(col(\"product_sku\"))\n",
    ")\n",
    "\n",
    "# Step 4: Remove duplicate order items\n",
    "order_items_deduped = order_items_with_sku.dropDuplicates([\"order_item_id\"])\n",
    "\n",
    "# Step 5: Add order_item_key hash column\n",
    "order_items_with_key = order_items_deduped.withColumn(\n",
    "    \"order_item_key\", \n",
    "    sha2(concat(col(\"order_item_id\"), col(\"order_id\"), col(\"product_sku\")), 256)\n",
    ")\n",
    "\n",
    "# Step 6: Add loaded_at timestamp\n",
    "order_items_silver_final = order_items_with_key.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "# Step 7: Write to Silver schema\n",
    "order_items_silver_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"jaffle_shop_retail.silver.order_items\")\n",
    "\n",
    "# Step 8: Display final result\n",
    "display(order_items_silver_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b889dff5-f743-4952-b1a1-82b152b504d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load products_bronze from Unity Catalog\n",
    "products_bronze = spark.table(\"jaffle_shop_retail.bronze.raw_products\")\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import col, when, lit, sha2, current_timestamp\n",
    "\n",
    "# Step 1: Select and cast columns to proper data types\n",
    "products_selected = products_bronze.select(\n",
    "    col(\"sku\").cast(StringType()).alias(\"sku\"),\n",
    "    col(\"name\").cast(StringType()).alias(\"product_name\"),\n",
    "    col(\"type\").cast(StringType()).alias(\"product_type\"),\n",
    "    col(\"price\").cast(IntegerType()).alias(\"price\"),\n",
    "    col(\"description\").cast(StringType()).alias(\"description\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter out null values from critical columns\n",
    "products_no_nulls = products_selected.filter(\n",
    "    col(\"sku\").isNotNull() &\n",
    "    col(\"product_name\").isNotNull() &\n",
    "    col(\"product_type\").isNotNull() &\n",
    "    col(\"price\").isNotNull()\n",
    ")\n",
    "\n",
    "# Step 3: Handle missing descriptions\n",
    "products_with_description = products_no_nulls.withColumn(\n",
    "    \"description\",\n",
    "    when(col(\"description\").isNull(), lit(\"No description available\"))\n",
    "    .otherwise(col(\"description\"))\n",
    ")\n",
    "\n",
    "# Step 4: Handle missing product types\n",
    "products_with_type = products_with_description.withColumn(\n",
    "    \"product_type\",\n",
    "    when(col(\"product_type\").isNull(), lit(\"unknown\"))\n",
    "    .otherwise(col(\"product_type\"))\n",
    ")\n",
    "\n",
    "# Step 5: Filter out invalid prices\n",
    "products_valid_price = products_with_type.filter(col(\"price\") > 0)\n",
    "\n",
    "# Step 6: Remove duplicate SKUs\n",
    "products_deduped = products_valid_price.dropDuplicates([\"sku\"])\n",
    "\n",
    "# Step 7: Add product_id hash column\n",
    "products_with_id = products_deduped.withColumn(\"product_id\", sha2(col(\"sku\"), 256))\n",
    "\n",
    "# Step 8: Add loaded_at timestamp\n",
    "products_silver_final = products_with_id.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "# Step 9: Write to Silver schema\n",
    "products_silver_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"jaffle_shop_retail.silver.products\")\n",
    "\n",
    "# Step 10: Display final result\n",
    "display(products_silver_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b027f60-fc98-467e-9227-a4349683395b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load stores_bronze from Unity Catalog\n",
    "stores_bronze = spark.table(\"jaffle_shop_retail.bronze.raw_stores\")\n",
    "\n",
    "from pyspark.sql.types import StringType, TimestampType, DoubleType\n",
    "from pyspark.sql.functions import col, when, lit, sha2, current_timestamp\n",
    "\n",
    "# Step 1: Select and cast columns to proper data types\n",
    "stores_selected = stores_bronze.select(\n",
    "    col(\"id\").cast(StringType()).alias(\"store_id\"),\n",
    "    col(\"name\").cast(StringType()).alias(\"store_name\"),\n",
    "    col(\"opened_at\").cast(TimestampType()).alias(\"opened_at\"),\n",
    "    col(\"tax_rate\").cast(DoubleType()).alias(\"tax_rate\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter out null values from critical columns\n",
    "stores_no_nulls = stores_selected.filter(\n",
    "    col(\"store_id\").isNotNull() &\n",
    "    col(\"store_name\").isNotNull() &\n",
    "    col(\"opened_at\").isNotNull()\n",
    ")\n",
    "\n",
    "# Step 3: Handle missing tax rates\n",
    "stores_with_tax_rate = stores_no_nulls.withColumn(\n",
    "    \"tax_rate\",\n",
    "    when(col(\"tax_rate\").isNull(), lit(0.0))\n",
    "    .otherwise(col(\"tax_rate\"))\n",
    ")\n",
    "\n",
    "# Step 4: Fix negative tax rates\n",
    "stores_valid_tax = stores_with_tax_rate.withColumn(\n",
    "    \"tax_rate\",\n",
    "    when(col(\"tax_rate\") < 0, lit(0.0))\n",
    "    .otherwise(col(\"tax_rate\"))\n",
    ")\n",
    "\n",
    "# Step 5: Normalize tax rates (convert percentages > 1 to decimal)\n",
    "stores_normalized_tax = stores_valid_tax.withColumn(\n",
    "    \"tax_rate\",\n",
    "    when(col(\"tax_rate\") > 1, col(\"tax_rate\") / 100)\n",
    "    .otherwise(col(\"tax_rate\"))\n",
    ")\n",
    "\n",
    "# Step 6: Remove duplicate stores\n",
    "stores_deduped = stores_normalized_tax.dropDuplicates([\"store_id\"])\n",
    "\n",
    "# Step 7: Add store_key hash column\n",
    "stores_with_key = stores_deduped.withColumn(\"store_key\", sha2(col(\"store_id\"), 256))\n",
    "\n",
    "# Step 8: Add loaded_at timestamp\n",
    "stores_silver_final = stores_with_key.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "# Step 9: Write to Silver schema\n",
    "stores_silver_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"jaffle_shop_retail.silver.stores\")\n",
    "\n",
    "# Step 10: Display final result\n",
    "display(stores_silver_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b7c014-7f1f-4b01-b439-4cbdebb3b482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load supplies_bronze from Unity Catalog\n",
    "supplies_bronze = spark.table(\"jaffle_shop_retail.bronze.raw_supplies\")\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, BooleanType\n",
    "from pyspark.sql.functions import col, when, lit, sha2, concat, current_timestamp\n",
    "\n",
    "# Step 1: Select and cast columns to proper data types\n",
    "supplies_selected = supplies_bronze.select(\n",
    "    col(\"id\").cast(StringType()).alias(\"supply_id\"),\n",
    "    col(\"name\").cast(StringType()).alias(\"supply_name\"),\n",
    "    col(\"cost\").cast(IntegerType()).alias(\"cost\"),\n",
    "    col(\"perishable\").cast(BooleanType()).alias(\"perishable\"),\n",
    "    col(\"sku\").cast(StringType()).alias(\"product_sku\")\n",
    ")\n",
    "\n",
    "# Step 2: Filter out null values from critical columns\n",
    "supplies_no_nulls = supplies_selected.filter(\n",
    "    col(\"supply_id\").isNotNull() &\n",
    "    col(\"supply_name\").isNotNull() &\n",
    "    col(\"cost\").isNotNull() &\n",
    "    col(\"product_sku\").isNotNull()\n",
    ")\n",
    "\n",
    "# Step 3: Handle missing perishable flags\n",
    "supplies_with_perishable = supplies_no_nulls.withColumn(\n",
    "    \"perishable\",\n",
    "    when(col(\"perishable\").isNull(), lit(False))\n",
    "    .otherwise(col(\"perishable\"))\n",
    ")\n",
    "\n",
    "# Step 4: Fix negative costs\n",
    "supplies_valid_cost = supplies_with_perishable.withColumn(\n",
    "    \"cost\",\n",
    "    when(col(\"cost\") < 0, lit(0))\n",
    "    .otherwise(col(\"cost\"))\n",
    ")\n",
    "\n",
    "# Step 5: Remove duplicate supply-SKU combinations\n",
    "supplies_deduped = supplies_valid_cost.dropDuplicates([\"supply_id\", \"product_sku\"])\n",
    "\n",
    "# Step 6: Add supply_key hash column\n",
    "supplies_with_key = supplies_deduped.withColumn(\n",
    "    \"supply_key\", \n",
    "    sha2(concat(col(\"supply_id\"), col(\"product_sku\")), 256)\n",
    ")\n",
    "\n",
    "# Step 7: Add loaded_at timestamp\n",
    "supplies_silver_final = supplies_with_key.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "# Step 8: Write to Silver schema\n",
    "supplies_silver_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"jaffle_shop_retail.silver.supplies\")\n",
    "\n",
    "# Step 9: Display final result\n",
    "display(supplies_silver_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47d4e51-0e9d-4681-b3e8-3c2a1b59e70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check all tables in silver schema\n",
    "display(spark.sql(\"SHOW TABLES IN jaffle_shop_retail.silver\"))\n",
    "\n",
    "# Verify schemas of each table\n",
    "print(\"Products Schema:\")\n",
    "display(spark.sql(\"DESCRIBE jaffle_shop_retail.silver.products\"))\n",
    "\n",
    "print(\"Stores Schema:\")\n",
    "display(spark.sql(\"DESCRIBE jaffle_shop_retail.silver.stores\"))\n",
    "\n",
    "print(\"Supplies Schema:\")\n",
    "display(spark.sql(\"DESCRIBE jaffle_shop_retail.silver.supplies\"))\n",
    "\n",
    "print(\"Customers Schema:\")\n",
    "display(spark.sql(\"DESCRIBE jaffle_shop_retail.silver.customers\"))\n",
    "\n",
    "print(\"Orders Schema:\")\n",
    "display(spark.sql(\"DESCRIBE jaffle_shop_retail.silver.orders\"))\n",
    "\n",
    "print(\"Order Items Schema:\")\n",
    "display(spark.sql(\"DESCRIBE jaffle_shop_retail.silver.order_items\"))\n",
    "\n",
    "# Count records in each silver table\n",
    "print(\"Record counts in silver tables:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT 'products' as table_name, COUNT(*) as count FROM jaffle_shop_retail.silver.products \n",
    "    UNION ALL \n",
    "    SELECT 'stores' as table_name, COUNT(*) as count FROM jaffle_shop_retail.silver.stores \n",
    "    UNION ALL \n",
    "    SELECT 'supplies' as table_name, COUNT(*) as count FROM jaffle_shop_retail.silver.supplies\n",
    "    UNION ALL\n",
    "    SELECT 'customers' as table_name, COUNT(*) as count FROM jaffle_shop_retail.silver.customers\n",
    "    UNION ALL\n",
    "    SELECT 'orders' as table_name, COUNT(*) as count FROM jaffle_shop_retail.silver.orders\n",
    "    UNION ALL\n",
    "    SELECT 'order_items' as table_name, COUNT(*) as count FROM jaffle_shop_retail.silver.order_items\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2a1d04-0663-4632-a8b2-66c6c528fa24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality checks after loading to silver\n",
    "from pyspark.sql.functions import count, when, isnan, isnull\n",
    "\n",
    "def check_data_quality(table_name):\n",
    "    df = spark.table(f\"jaffle_shop_retail.silver.{table_name}\")\n",
    "    total_count = df.count()\n",
    "    null_counts = df.select([\n",
    "        count(when(isnull(c), c)).alias(c) for c in df.columns\n",
    "    ])\n",
    "    print(f\"Data Quality Check for {table_name}:\")\n",
    "    print(f\"Total records: {total_count}\")\n",
    "    print(\"Null counts per column:\")\n",
    "    display(null_counts)\n",
    "    return total_count\n",
    "\n",
    "# Run quality checks for all tables\n",
    "for table in [\"products\", \"stores\", \"supplies\", \"customers\", \"orders\", \"order_items\"]:\n",
    "    check_data_quality(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d070742-64dd-42f0-96e7-1ce9da405e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# UNIT TESTING FOR ALL 6 SILVER TABLES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b4f0844-cd61-47be-a32e-c6fed84a9e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import testing dependencies\n",
    "import unittest\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Test class for Customers Silver Layer\n",
    "class TestCustomersSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df = spark.table(\"jaffle_shop_retail.silver.customers\")\n",
    "\n",
    "    def test_customers_schema(self):\n",
    "        expected_columns = ['customer_id', 'customer_name', 'customer_key', 'loaded_at']\n",
    "        self.assertListEqual(self.df.columns, expected_columns)\n",
    "\n",
    "    def test_customers_no_null_ids(self):\n",
    "        null_count = self.df.filter(col(\"customer_id\").isNull()).count()\n",
    "        self.assertEqual(null_count, 0)\n",
    "\n",
    "    def test_customers_no_duplicate_ids(self):\n",
    "        duplicate_count = self.df.groupBy(\"customer_id\").count().filter(col(\"count\") > 1).count()\n",
    "        self.assertEqual(duplicate_count, 0)\n",
    "\n",
    "    def test_customers_valid_names(self):\n",
    "        invalid_names = self.df.filter(\n",
    "            (col(\"customer_name\").isNull()) |\n",
    "            (trim(col(\"customer_name\")) == \"\") |\n",
    "            (length(trim(col(\"customer_name\"))) < 2)\n",
    "        )\n",
    "        self.assertEqual(invalid_names.count(), 0)\n",
    "\n",
    "    def test_customers_has_records(self):\n",
    "        self.assertGreater(self.df.count(), 0)\n",
    "\n",
    "# Test class for Orders Silver Layer\n",
    "class TestOrdersSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df = spark.table(\"jaffle_shop_retail.silver.orders\")\n",
    "\n",
    "    def test_orders_schema(self):\n",
    "        expected_columns = ['order_id', 'customer_id', 'ordered_at', 'store_id',\n",
    "                            'subtotal', 'tax_paid', 'order_total', 'order_key', 'loaded_at']\n",
    "        self.assertListEqual(self.df.columns, expected_columns)\n",
    "\n",
    "    def test_orders_financial_validation(self):\n",
    "        invalid_financials = self.df.filter(\n",
    "            (col(\"subtotal\") < 0) |\n",
    "            (col(\"tax_paid\") < 0) |\n",
    "            (col(\"order_total\") < 0)\n",
    "        )\n",
    "        self.assertEqual(invalid_financials.count(), 0)\n",
    "\n",
    "    def test_orders_no_null_critical_fields(self):\n",
    "        null_count = self.df.filter(\n",
    "            col(\"order_id\").isNull() |\n",
    "            col(\"customer_id\").isNull() |\n",
    "            col(\"ordered_at\").isNull() |\n",
    "            col(\"store_id\").isNull()\n",
    "        ).count()\n",
    "        self.assertEqual(null_count, 0)\n",
    "\n",
    "    def test_orders_has_records(self):\n",
    "        self.assertGreater(self.df.count(), 0)\n",
    "\n",
    "# Test class for Products Silver Layer\n",
    "class TestProductsSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df = spark.table(\"jaffle_shop_retail.silver.products\")\n",
    "\n",
    "    def test_products_schema(self):\n",
    "        expected_columns = ['sku', 'product_name', 'product_type', 'price',\n",
    "                            'description', 'product_id', 'loaded_at']\n",
    "        self.assertListEqual(self.df.columns, expected_columns)\n",
    "\n",
    "    def test_products_positive_prices(self):\n",
    "        non_positive_prices = self.df.filter(col(\"price\") <= 0)\n",
    "        self.assertEqual(non_positive_prices.count(), 0)\n",
    "\n",
    "    def test_products_no_duplicate_skus(self):\n",
    "        duplicate_count = self.df.groupBy(\"sku\").count().filter(col(\"count\") > 1).count()\n",
    "        self.assertEqual(duplicate_count, 0)\n",
    "\n",
    "    def test_products_has_records(self):\n",
    "        self.assertGreater(self.df.count(), 0)\n",
    "\n",
    "# Test class for Order Items Silver Layer\n",
    "class TestOrderItemsSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df = spark.table(\"jaffle_shop_retail.silver.order_items\")\n",
    "\n",
    "    def test_order_items_schema(self):\n",
    "        expected_columns = ['order_item_id', 'order_id', 'product_sku', 'order_item_key', 'loaded_at']\n",
    "        self.assertListEqual(self.df.columns, expected_columns)\n",
    "\n",
    "    def test_order_items_no_null_fields(self):\n",
    "        null_count = self.df.filter(\n",
    "            col(\"order_item_id\").isNull() |\n",
    "            col(\"order_id\").isNull() |\n",
    "            col(\"product_sku\").isNull()\n",
    "        ).count()\n",
    "        self.assertEqual(null_count, 0)\n",
    "\n",
    "    def test_order_items_has_records(self):\n",
    "        self.assertGreater(self.df.count(), 0)\n",
    "\n",
    "# Test class for Stores Silver Layer\n",
    "class TestStoresSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df = spark.table(\"jaffle_shop_retail.silver.stores\")\n",
    "\n",
    "    def test_stores_schema(self):\n",
    "        expected_columns = ['store_id', 'store_name', 'opened_at', 'tax_rate', 'store_key', 'loaded_at']\n",
    "        self.assertListEqual(self.df.columns, expected_columns)\n",
    "\n",
    "    def test_stores_valid_tax_rates(self):\n",
    "        invalid_tax_rates = self.df.filter(\n",
    "            (col(\"tax_rate\") < 0) | (col(\"tax_rate\") > 1)\n",
    "        )\n",
    "        self.assertEqual(invalid_tax_rates.count(), 0)\n",
    "\n",
    "    def test_stores_no_duplicate_ids(self):\n",
    "        duplicate_count = self.df.groupBy(\"store_id\").count().filter(col(\"count\") > 1).count()\n",
    "        self.assertEqual(duplicate_count, 0)\n",
    "\n",
    "    def test_stores_has_records(self):\n",
    "        self.assertGreater(self.df.count(), 0)\n",
    "\n",
    "# Test class for Supplies Silver Layer\n",
    "class TestSuppliesSilver(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.df = spark.table(\"jaffle_shop_retail.silver.supplies\")\n",
    "\n",
    "    def test_supplies_schema(self):\n",
    "        expected_columns = ['supply_id', 'supply_name', 'cost', 'perishable',\n",
    "                            'product_sku', 'supply_key', 'loaded_at']\n",
    "        self.assertListEqual(self.df.columns, expected_columns)\n",
    "\n",
    "    def test_supplies_non_negative_cost(self):\n",
    "        negative_costs = self.df.filter(col(\"cost\") < 0)\n",
    "        self.assertEqual(negative_costs.count(), 0)\n",
    "\n",
    "    def test_supplies_no_duplicate_supply_sku(self):\n",
    "        duplicate_count = self.df.groupBy(\"supply_id\", \"product_sku\").count().filter(col(\"count\") > 1).count()\n",
    "        self.assertEqual(duplicate_count, 0)\n",
    "\n",
    "    def test_supplies_has_records(self):\n",
    "        self.assertGreater(self.df.count(), 0)\n",
    "\n",
    "# Integration Tests for all 6 tables\n",
    "class TestIntegrationSilver(unittest.TestCase):\n",
    "\n",
    "    def test_orders_customers_integration(self):\n",
    "        \"\"\"Test that all orders have valid customer references\"\"\"\n",
    "        orders_df = spark.table(\"jaffle_shop_retail.silver.orders\")\n",
    "        customers_df = spark.table(\"jaffle_shop_retail.silver.customers\")\n",
    "\n",
    "        invalid_orders = orders_df.join(\n",
    "            customers_df,\n",
    "            orders_df.customer_id == customers_df.customer_id,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        self.assertEqual(invalid_orders.count(), 0, \"All orders should have valid customer IDs\")\n",
    "\n",
    "    def test_order_items_orders_integration(self):\n",
    "        \"\"\"Test that all order items have valid order references\"\"\"\n",
    "        order_items_df = spark.table(\"jaffle_shop_retail.silver.order_items\")\n",
    "        orders_df = spark.table(\"jaffle_shop_retail.silver.orders\")\n",
    "\n",
    "        invalid_order_items = order_items_df.join(\n",
    "            orders_df,\n",
    "            order_items_df.order_id == orders_df.order_id,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        self.assertEqual(invalid_order_items.count(), 0, \"All order items should have valid order IDs\")\n",
    "\n",
    "    def test_order_items_products_integration(self):\n",
    "        \"\"\"Test that all order items have valid product SKU references\"\"\"\n",
    "        order_items_df = spark.table(\"jaffle_shop_retail.silver.order_items\")\n",
    "        products_df = spark.table(\"jaffle_shop_retail.silver.products\")\n",
    "\n",
    "        invalid_order_items = order_items_df.join(\n",
    "            products_df,\n",
    "            order_items_df.product_sku == products_df.sku,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        # This might have some if there are new products not in products table\n",
    "        print(f\"Order items with invalid product SKUs: {invalid_order_items.count()}\")\n",
    "\n",
    "    def test_orders_stores_integration(self):\n",
    "        \"\"\"Test that all orders have valid store references\"\"\"\n",
    "        orders_df = spark.table(\"jaffle_shop_retail.silver.orders\")\n",
    "        stores_df = spark.table(\"jaffle_shop_retail.silver.stores\")\n",
    "\n",
    "        invalid_orders = orders_df.join(\n",
    "            stores_df,\n",
    "            orders_df.store_id == stores_df.store_id,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        self.assertEqual(invalid_orders.count(), 0, \"All orders should have valid store IDs\")\n",
    "\n",
    "    def test_supplies_products_integration(self):\n",
    "        \"\"\"Test that all supplies have valid product SKU references\"\"\"\n",
    "        supplies_df = spark.table(\"jaffle_shop_retail.silver.supplies\")\n",
    "        products_df = spark.table(\"jaffle_shop_retail.silver.products\")\n",
    "\n",
    "        invalid_supplies = supplies_df.join(\n",
    "            products_df,\n",
    "            supplies_df.product_sku == products_df.sku,\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        # This might have some if there are supplies for products not in products table\n",
    "        print(f\"Supplies with invalid product SKUs: {invalid_supplies.count()}\")\n",
    "\n",
    "# Data Quality Tests for all tables\n",
    "class TestDataQualitySilver(unittest.TestCase):\n",
    "\n",
    "    def test_all_6_tables_have_records(self):\n",
    "        \"\"\"Test that all 6 silver tables have records\"\"\"\n",
    "        tables = [\"customers\", \"orders\", \"order_items\", \"products\", \"stores\", \"supplies\"]\n",
    "\n",
    "        for table in tables:\n",
    "            count = spark.table(f\"jaffle_shop_retail.silver.{table}\").count()\n",
    "            self.assertGreater(count, 0, f\"Table {table} should have records\")\n",
    "            print(f\"âœ“ {table}: {count} records\")\n",
    "\n",
    "    def test_all_tables_have_loaded_at(self):\n",
    "        \"\"\"Test that all 6 tables have loaded_at timestamp\"\"\"\n",
    "        tables = [\"customers\", \"orders\", \"order_items\", \"products\", \"stores\", \"supplies\"]\n",
    "\n",
    "        for table in tables:\n",
    "            df = spark.table(f\"jaffle_shop_retail.silver.{table}\")\n",
    "            null_loaded_at = df.filter(col(\"loaded_at\").isNull()).count()\n",
    "            self.assertEqual(null_loaded_at, 0, f\"Table {table} should have loaded_at timestamps\")\n",
    "\n",
    "    def test_all_tables_have_hash_keys(self):\n",
    "        \"\"\"Test that all tables have their respective hash keys\"\"\"\n",
    "        table_key_mapping = {\n",
    "            \"customers\": \"customer_key\",\n",
    "            \"orders\": \"order_key\",\n",
    "            \"order_items\": \"order_item_key\",\n",
    "            \"products\": \"product_id\",\n",
    "            \"stores\": \"store_key\",\n",
    "            \"supplies\": \"supply_key\"\n",
    "        }\n",
    "\n",
    "        for table, key_column in table_key_mapping.items():\n",
    "            df = spark.table(f\"jaffle_shop_retail.silver.{table}\")\n",
    "            null_keys = df.filter(col(key_column).isNull()).count()\n",
    "            self.assertEqual(null_keys, 0, f\"Table {table} should have non-null {key_column}\")\n",
    "\n",
    "integration_test = unittest.TestLoader().loadTestsFromTestCase(TestIntegrationSilver)\n",
    "unittest.TextTestRunner(verbosity=1).run(integration_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a40ca56-5b98-49bc-9d19-e85d059895c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_table_tests(table_name):\n",
    "    try:\n",
    "        df = spark.table(table_name)\n",
    "        display(\n",
    "            df.selectExpr(\n",
    "                f\"'{table_name}' as table_name\",\n",
    "                \"count(*) as row_count\"\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Table not found or error for {table_name}: {e}\")\n",
    "\n",
    "run_table_tests(\"jaffle_shop_retail.silver.customers\")\n",
    "run_table_tests(\"jaffle_shop_retail.silver.orders\")\n",
    "run_table_tests(\"jaffle_shop_retail.silver.order_items\")\n",
    "run_table_tests(\"jaffle_shop_retail.silver.supplies\")\n",
    "run_table_tests(\"jaffle_shop_retail.silver.stores\")\n",
    "run_table_tests(\"jaffle_shop_retail.silver.products\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
